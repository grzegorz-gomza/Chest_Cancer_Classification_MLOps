{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import kaggle\n",
    "import BreastCancerClassifier.constants\n",
    "from BreastCancerClassifier.utils.common import read_yaml, create_directories\n",
    "\n",
    "# Load environment variables from a file .env\n",
    "load_dotenv()  # Be sure to set the .env file beforehand\n",
    "\n",
    "# Get the path to the Kaggle API configuration directory from the .env file\n",
    "kaggle_config_dir = os.getenv(\"KAGGLE_CONFIG_DIR\")\n",
    "\n",
    "if kaggle_config_dir:\n",
    "    # Setting the environment variable for the Kaggle API configuration directory\n",
    "    os.environ['KAGGLE_CONFIG_DIR'] = kaggle_config_dir\n",
    "    print(f'Path to Kaggle API configuration directory: {kaggle_config_dir}')\n",
    "else:\n",
    "    raise ValueError(\"The KAGGLE_CONFIG_DIR variable is not set in the .env file\")\n",
    "\n",
    "def download_kaggle_dataset(dataset, download_path='.'):\n",
    "    try:\n",
    "        \n",
    "        kaggle.api.authenticate()\n",
    "        kaggle.api.dataset_download_files(dataset, path=download_path, unzip=True)\n",
    "        print(f'Dataset {dataset} has been successfully downloaded and saved in: {download_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error while downloading: {e}')\n",
    "\n",
    "# Function to check if the dataset has already been downloaded\n",
    "def is_dataset_downloaded(download_path):\n",
    "    if os.path.exists(download_path) and len(os.listdir(download_path)) > 0:\n",
    "        print(\"Dataset is already downloaded.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Dataset is not downloaded. Proceeding to download.\")\n",
    "        return False\n",
    "\n",
    "# Definition of the dataset and download path\n",
    "data_set = 'ronanpickell/b200c-lego-classification-dataset'\n",
    "download_path = 'data/lego-dataset'  # Path must comply with Linux/WSL\n",
    "\n",
    "# Download dataset if not already downloaded\n",
    "if not is_dataset_downloaded(download_path):\n",
    "    api.dataset_download_files(dataset, path=download_path, unzip=True)\n",
    "    print(\"Dataset downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mohamedhanyyy/chest-ctscan-images\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration.username = 'your_user_name'\n",
    "configuration.password = 'your_password'\n",
    "KAGGLE_CONFIG_DIR='/mnt/Your_Path/.kaggle' # For Ubuntu virtual enviroment\n",
    "# KAGGLE_CONFIG_DIR='/Your_Path/.kaggle' # for Windows virtual enviroment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SCRIPT_DIR = Path().resolve()  # Resolves to the current working directory\n",
    "CONFIG_FILE_PATH = SCRIPT_DIR / \"config\" / \"config.yaml\"\n",
    "PARAMS_FILE_PATH = SCRIPT_DIR / \"params.yaml\"\n",
    "\n",
    "print(SCRIPT_DIR)  # See what the current working directory is\n",
    "print(CONFIG_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from box import ConfigBox  # For using Box configurations \n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\") # set constant path to config.yaml\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\") # set constant path to params.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "# Create a configuration manager for data ingestion\n",
    "from BreastCancerClassifier.utils.common import read_yaml, create_directories\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    \"\"\"Data Ingestion configuration.\"\"\"\n",
    "    root_dir: Path\n",
    "    kaggle_source: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-25 13:11:10,090]: INFO: common: read_yaml: yaml file: config/config.yaml loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "box.config_box.ConfigBox"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_filepath = CONFIG_FILE_PATH\n",
    "config = read_yaml(config_filepath)\n",
    "type(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from BreastCancerClassifier.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"Manages project configurations, including data ingestion settings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath:  Path = CONFIG_FILE_PATH, # Path to configuration file - importet from constants\n",
    "        params_filepath:  Path = PARAMS_FILE_PATH  # Path to parameters file - importet from constants\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager.\n",
    "\n",
    "        Args:\n",
    "            config_filepath: Path to the configuration YAML file.\n",
    "            params_filepath: Path to the parameters YAML file.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        #Check if artifacts_root exist inside self.config\n",
    "        if 'artifacts_root' not in self.config:\n",
    "          raise KeyError(\"'artifacts_root' key not found in the configuration file.\")\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) #Use dictionary acces to avoid future errors.\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Retrieves and constructs the data ingestion configuration.\n",
    "\n",
    "        Returns:\n",
    "            A DataIngestionConfig object.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If necessary keys are missing from the configuration.\n",
    "        \"\"\"\n",
    "\n",
    "        #Error Handling - Check if 'data_ingestion' exist before accessing.\n",
    "        if 'data_ingestion' not in self.config:\n",
    "            raise KeyError(\"'data_ingestion' key not found in the configuration file.\")\n",
    "\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        # Error Handling: Check if necessary keys are present within 'data_ingestion'\n",
    "        required_keys = ['root_dir', 'kaggle_source']\n",
    "        for key in required_keys:\n",
    "            if key not in config:\n",
    "                raise KeyError(f\"'{key}' key not found in 'data_ingestion' section of the configuration file.\")\n",
    "\n",
    "        #Type validation\n",
    "        if not isinstance(config['root_dir'], str):\n",
    "             raise TypeError(\"'root_dir' in 'data_ingestion' must be a string.\")\n",
    "        if not isinstance(config['kaggle_source'], str):\n",
    "             raise TypeError(\"'kaggle_source' in 'data_ingestion' must be a string.\")\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        #Adding default parameters.\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            kaggle_source=config.kaggle_source,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/DataScience/Documents/GitHub/Chest_Cancer_Classification_MLOps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import kaggle\n",
    "from BreastCancerClassifier import logger\n",
    "from BreastCancerClassifier.utils.common import get_size\n",
    "\n",
    "\n",
    "class DataIngestion:\n",
    "    \"\"\"\n",
    "    Handles the downloading of datasets for the Breast Cancer Classifier project,\n",
    "    including validations, logging, and data size calculations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialization with configuration provided.\n",
    "\n",
    "        :param config: Configuration object for data ingestion.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def is_dataset_downloaded(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the dataset has already been downloaded.\n",
    "\n",
    "        :return: True if dataset exists and is non-empty, False otherwise.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.config.root_dir) and len(os.listdir(self.config.root_dir)) > 0:\n",
    "            logger.info(\"Dataset is already downloaded.\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.info(\"Dataset is not downloaded. Proceeding to download.\")\n",
    "            return False\n",
    "\n",
    "    def validate_kaggle_credentials(self) -> None:\n",
    "        \"\"\"\n",
    "        Validate Kaggle API credentials. Logs an error and raises an exception if credentials are missing.\n",
    "        \"\"\"\n",
    "        dotenv.load_dotenv()\n",
    "\n",
    "        kaggle_username = os.getenv(\"KAGGLE_USERNAME\")\n",
    "        kaggle_key = os.getenv(\"KAGGLE_KEY\")\n",
    "\n",
    "        if not kaggle_username or not kaggle_key:\n",
    "            error_message = \"\"\"\n",
    "            Kaggle API credentials not found. Ensure the following environment variables are set:\n",
    "              - KAGGLE_USERNAME\n",
    "              - KAGGLE_KEY\n",
    "            Add your credentials to a `.env` file or export them as environment variables.\n",
    "            \"\"\"\n",
    "            logger.error(error_message)\n",
    "            raise EnvironmentError(error_message)\n",
    "\n",
    "        logger.info(\"Kaggle API credentials imported successfully.\")\n",
    "\n",
    "    def validate_config(self) -> None:\n",
    "        \"\"\"\n",
    "        Validate the essential configurations from the config file.\n",
    "        \"\"\"\n",
    "        if not self.config.kaggle_source:\n",
    "            error_message = \"The 'kaggle_source' variable is missing in the configuration file.\"\n",
    "            logger.error(error_message)\n",
    "            raise ValueError(error_message)\n",
    "\n",
    "        if not os.path.exists(self.config.root_dir):\n",
    "            logger.info(f\"Root directory '{self.config.root_dir}' does not exist. Creating it.\")\n",
    "            os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "\n",
    "    def download_file(self) -> None:\n",
    "        \"\"\"\n",
    "        Handles dataset download from Kaggle and ensures it is available locally.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting dataset download process...\")\n",
    "        self.validate_config()\n",
    "\n",
    "        # Check if dataset is already downloaded\n",
    "        if self.is_dataset_downloaded():\n",
    "            dataset_size = get_size(Path(self.config.root_dir))\n",
    "            logger.info(f\"Dataset already downloaded. Size: {dataset_size} kB\")\n",
    "            return\n",
    "\n",
    "        # Validate Kaggle API credentials\n",
    "        self.validate_kaggle_credentials()\n",
    "\n",
    "        # Attempt to download the dataset\n",
    "        try:\n",
    "            logger.info(f\"Downloading dataset from Kaggle: {self.config.kaggle_source}\")\n",
    "            kaggle.api.authenticate()\n",
    "            kaggle.api.dataset_download_files(\n",
    "                dataset=self.config.kaggle_source,\n",
    "                path=self.config.root_dir,\n",
    "                unzip=True\n",
    "            )\n",
    "            logger.info(f\"Dataset '{self.config.kaggle_source}' has been successfully downloaded to '{self.config.root_dir}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred while downloading the dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Verify download\n",
    "        if self.is_dataset_downloaded():\n",
    "            dataset_size = get_size(Path(self.config.root_dir))\n",
    "            logger.info(f\"Dataset download complete. Size: {dataset_size} kB.\")\n",
    "        else:\n",
    "            error_message = f\"Dataset download failed. The directory '{self.config.root_dir}' is empty.\"\n",
    "            logger.error(error_message)\n",
    "            raise RuntimeError(error_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifacts/data_ingestion/.kaggle'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion_config.root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file size: ~ 4 KB'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_size(Path(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-25 13:22:07,734]: INFO: common: read_yaml: yaml file: config/config.yaml loaded successfully!\n",
      "<class 'box.config_box.ConfigBox'>\n",
      "<class 'str'>\n",
      "artifacts\n",
      "[2025-02-25 13:22:07,736]: INFO: common: create_directories: Created directory at: artifacts\n",
      "[2025-02-25 13:22:07,740]: INFO: common: create_directories: Created directory at: artifacts/data_ingestion/.kaggle\n",
      "[2025-02-25 13:22:07,741]: INFO: 3773441251: download_file: Starting dataset download process...\n",
      "[2025-02-25 13:22:07,746]: INFO: 3773441251: is_dataset_downloaded: Dataset is already downloaded.\n",
      "[2025-02-25 13:22:07,748]: INFO: 3773441251: download_file: Dataset already downloaded. Size: file size: ~ 4 KB kB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.download_file()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lengthl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m     stage_start \u001b[38;5;241m=\u001b[39m dedent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39msymbol\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;241m.\u001b[39mcenter(lengthl)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39msymbol\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stage_start\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstart_stage_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTAGE_NAME\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m, in \u001b[0;36mstart_stage_logger\u001b[0;34m(stage_name, length, symbol)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart_stage_logger\u001b[39m(stage_name: \u001b[38;5;28mstr\u001b[39m, length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m, symbol :\u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      6\u001b[0m     stage_start \u001b[38;5;241m=\u001b[39m dedent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39msymbol\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;241m.\u001b[39mcenter(\u001b[43mlengthl\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39msymbol\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stage_start\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lengthl' is not defined"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "STAGE_NAME = \"Data Ingestion stage\"\n",
    "\n",
    "\n",
    "def start_stage_logger(stage_name: str, length: int = 40, symbol :str = \"#\") -> str:\n",
    "    stage_start = dedent(f\"\"\"\\\n",
    "        {length * symbol}\n",
    "        {stage_name.upper().center(lengthl)} \n",
    "        {length * symbol}\n",
    "        \"\"\")\n",
    "    return stage_start\n",
    "\n",
    "print(start_stage_logger(STAGE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e abc f abc g'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" abc \".join(\"efg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m########################################\n",
      "      DATA INGESTION STAGE STARTED      \n",
      "########################################        \n",
      " Data Ingestion stagesData Ingestion stagetData Ingestion stageaData Ingestion stagerData Ingestion stagetData Ingestion stageeData Ingestion staged\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "STAGE_NAME = \"Data Ingestion stage\"\n",
    "\n",
    "\n",
    "def start_stage_logger(stage_name: str, length: int = 40, symbol: str = \"#\") -> str:\n",
    "\n",
    "    # ANSI escape codes for color\n",
    "    GREEN = '\\033[92m'  # Green\n",
    "\n",
    "    stage_name_start = \"\".join([STAGE_NAME, \" started\"])\n",
    "    \n",
    "    stage_start = dedent(f\"\"\"\\\n",
    "        {GREEN}{length * symbol}\n",
    "        {stage_name_start.upper().center(length,\" \")}\n",
    "        {length * symbol}\\\n",
    "        \"\"\")\n",
    "    return stage_start\n",
    "\n",
    "print(start_stage_logger(STAGE_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
